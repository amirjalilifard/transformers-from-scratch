{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.txt\", delimiter=\",\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data[:, 0], data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    d = {}\n",
    "    pattern = r\"[^\\w\\s]|\\.{1,}\"\n",
    "    word_list = list(\n",
    "        set(\n",
    "            [\n",
    "                re.sub(pattern, \"\", word)\n",
    "                for sublist in data\n",
    "                for word in sublist.split(\" \")\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    for i, word in enumerate(word_list):\n",
    "        d[word] = i\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dictionary = tokenize(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding:\n",
    "\n",
    "    def __init__(self, max_length, embedding_dim, lookup_dictionary):\n",
    "        self.embedding_matrix = np.zeros((max_length, embedding_dim))\n",
    "        self.max_length = max_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lookup_dictionary = lookup_dictionary\n",
    "\n",
    "    def generate(self, sentence):\n",
    "        indices = None\n",
    "        try:\n",
    "            indices = [self.lookup_dictionary[word] for word in sentence]\n",
    "        except:\n",
    "            raise Exception(\"new word was identified\")\n",
    "\n",
    "        sentence_pos_embeddings = []\n",
    "        for pos in range(len(sentence)):\n",
    "            word_pos_embedding = np.zeros(self.embedding_dim)\n",
    "            for dim in range(self.embedding_dim):\n",
    "                if dim % 2:\n",
    "                    word_pos_embedding[dim] = self.odd_PE(pos, dim)\n",
    "                else:\n",
    "                    word_pos_embedding[dim] = self.even_PE(pos, dim)\n",
    "            sentence_pos_embeddings.append(word_pos_embedding.copy())\n",
    "        return sentence_pos_embeddings, indices\n",
    "\n",
    "    def even_PE(self, pos, i):\n",
    "        return math.cos(pos / (10000 ** ((2 * i) / self.embedding_dim)))\n",
    "\n",
    "    def odd_PE(self, pos, i):\n",
    "        return math.sin(pos / (10000 ** ((2 * i) / self.embedding_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"its easy to carry\".split()\n",
    "embedding = PositionalEmbedding(lookup_dictionary= lookup_dictionary , max_length=100, embedding_dim=10)\n",
    "pos_embeddings, indices = embedding.generate(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_heads, indices, masking = False):\n",
    "        ## Input_dim is basically the dimension of the positional word embeddiing\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.key_linear = nn.Linear(input_dim, output_dim)\n",
    "        self.value_linear = nn.Linear(input_dim, output_dim)\n",
    "        self.query_linear = nn.Linear(input_dim, output_dim)\n",
    "        self.last_linear = nn.Linear(input_dim, output_dim)\n",
    "        self.indices = torch.tensor(indices)\n",
    "        self.depth = self.output_dim // self.num_heads\n",
    "        self.masking = masking\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "\n",
    "        reshaped_x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return reshaped_x.transpose(1, 2)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size = 1\n",
    "\n",
    "        K_out = self.key_linear(K)\n",
    "        Q_out = self.query_linear(Q)\n",
    "        V_out = self.value_linear(V)\n",
    "\n",
    "        K_splitted = self.split_heads(K_out, batch_size)\n",
    "        Q_splitted = self.split_heads(Q_out, batch_size)\n",
    "        V_splitted = self.split_heads(V_out, batch_size)\n",
    "\n",
    "        attention_weight = torch.matmul(\n",
    "            Q_splitted, K_splitted.transpose(-2, -1)\n",
    "        ) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n",
    "\n",
    "        # Here the attention_weight has the shape (batch size, number of heads, number of words in the sentence, number of heads, number of words in the sentence)\n",
    "\n",
    "        ## up until now, we have calculated the attention scores. Now, we should apply the masking if True\n",
    "\n",
    "        if self.masking:\n",
    "            trinagular_matrix = torch.tril(\n",
    "                torch.ones([attention_weight.shape[2], attention_weight.shape[2]])\n",
    "            )\n",
    "            trinagular_matrix = trinagular_matrix.unsqueeze(0).unsqueeze(0)\n",
    "            trinagular_matrix = trinagular_matrix.expand(\n",
    "                batch_size,\n",
    "                self.num_heads,\n",
    "                attention_weight.shape[2],\n",
    "                attention_weight.shape[2],\n",
    "            )\n",
    "            attention_weight = attention_weight.masked_fill(trinagular_matrix == 0, float('-inf'))\n",
    "\n",
    "        normalized_attention_weight = F.softmax(attention_weight, -1)\n",
    "        \n",
    "        output = torch.matmul(normalized_attention_weight, V_splitted)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, -1, self.output_dim)\n",
    "\n",
    "        output = self.last_linear(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4993, 0.5007, 0.0000, 0.0000],\n",
      "          [0.3410, 0.3344, 0.3246, 0.0000],\n",
      "          [0.2656, 0.2574, 0.2429, 0.2341]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4866, 0.5134, 0.0000, 0.0000],\n",
      "          [0.3276, 0.3309, 0.3415, 0.0000],\n",
      "          [0.2570, 0.2529, 0.2471, 0.2429]]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[[-4.5705e-02,  1.9059e-01, -1.9999e-01,  1.2057e-01, -2.6404e-01],\n",
      "          [ 7.6685e-04,  1.2746e-01, -1.6928e-01,  9.2891e-02, -2.0819e-01],\n",
      "          [ 6.6716e-02,  1.7018e-02, -1.0096e-01,  6.0378e-02, -1.2653e-01],\n",
      "          [ 1.2254e-01, -7.2927e-02, -4.7223e-02,  3.1676e-02, -5.7822e-02]],\n",
      "\n",
      "         [[ 5.5688e-01, -1.3704e-01, -4.2619e-01,  1.0696e+00, -2.3870e-01],\n",
      "          [ 4.9962e-01, -1.6114e-01, -3.8849e-01,  1.0126e+00, -2.0362e-01],\n",
      "          [ 3.9732e-01, -2.1184e-01, -3.0959e-01,  9.3333e-01, -1.2787e-01],\n",
      "          [ 3.1769e-01, -2.5027e-01, -2.4972e-01,  8.6859e-01, -7.0673e-02]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "x = torch.tensor(pos_embeddings)\n",
    "\n",
    "seq_len = x.shape[0]\n",
    "input_dim = x.shape[1]\n",
    "output_dim = x.shape[1]\n",
    "num_heads = 2\n",
    "\n",
    "\n",
    "attention = MultiHeadAttention(input_dim, output_dim, num_heads, indices, masking= True)\n",
    "output = attention(x, x, x)\n",
    "# print(output)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, input_dim, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = input_dim\n",
    "        self.linear1 = nn.Linear(input_dim, self.output_dim)\n",
    "        self.linear2 = nn.Linear(self.output_dim, self.output_dim)\n",
    "        self.linear3 = nn.Linear(self.output_dim, self.output_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.input_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.output_dim)\n",
    "        self.layer_norm3 = nn.LayerNorm(self.output_dim)\n",
    "\n",
    "    def forward(self, multihead_att_out):\n",
    "        # If the multihead_att_out has the dimension (batch_size, number of words in the sentence, number of dimensions) then the layernorm should\n",
    "        #    be done over the number of dimensions\n",
    "\n",
    "        # As you can see, the encoder gets the output of the attention layer and passes through a set \n",
    "        #of linear tranformations with residual layers and layernorm\n",
    "        x_res_1 = F.relu(self.linear1(multihead_att_out)) + multihead_att_out\n",
    "        x_out_first_layer = self.layer_norm1(x_res_1)\n",
    "\n",
    "        x_res_2 = F.relu(self.linear2(x_out_first_layer)) + x_out_first_layer\n",
    "        x_out_second_layer = self.layer_norm2(x_res_2)\n",
    "\n",
    "        x_res_3 = F.relu(self.linear3(x_out_second_layer)) + x_out_second_layer\n",
    "        x_out_third_layer = self.layer_norm3(x_res_3)\n",
    "\n",
    "        return x_out_third_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTransformer(nn.Module):\n",
    "    def __init__(self, num_encoders, input_dim, output_dim, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        encoder_list = [EncoderLayer(input_dim) for i in range(num_encoders)]\n",
    "        self.stacked_encoders = nn.ModuleList([encoder for encoder in encoder_list])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for encoder in self.stacked_encoders:\n",
    "            x = encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_transformer = EncoderTransformer(num_encoders= 3, input_dim= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_out = encoder_transformer(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.linear1 = nn.Linear(input_dim, output_dim)\n",
    "        self.linear2 = nn.Linear(input_dim, output_dim)\n",
    "        self.linear3 = nn.Linear(input_dim, output_dim)\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(\n",
    "            input_dim = input_dim, output_dim = output_dim, masking = False\n",
    "        )\n",
    "\n",
    "        self.self_decoder_attention = MultiHeadAttention(\n",
    "            input_dim=input_dim, output_dim=output_dim, masking = True\n",
    "        )\n",
    "\n",
    "    def forward(self, encoders_multihead_att_out, target_sequence_position_embedding):\n",
    "        ## Here we should implement the residual nets but before we need to create self and encoder-decoder attentions. Here is the idea:\n",
    "        \"\"\"\n",
    "                In the decoder, first we need to create a self attention for the \"oi tudo bem\" to have a context rich embedding for this sentence,\n",
    "                also to have an attention map for the translated sentence.\n",
    "                Then, we use this self attention output to generate another attention.\n",
    "                This time, we want to generate the attention map of both \"oi tudo bem\" and \"how are you\",\n",
    "                saying: knowing who I am in the translated sentence, to whom in the otiginal sentence should I give attention.\n",
    "                The output of this second multihead attention is a context rich embedding in both languages.\n",
    "                Here is a summary:\n",
    "                \n",
    "                Self-Attention for Target Sequence:\n",
    "                Generate a context-rich embedding for the translated sentence (\"oi tudo bem\").\n",
    "                Create an attention map for the translated sentence itself.\n",
    "                \n",
    "                Encoder-Decoder Attention:\n",
    "                Use the self-attention output (context-rich embedding of \"oi tudo bem\") as queries.\n",
    "                Use the encoder's output (context-rich embedding of \"hey how are you\") as keys and values.\n",
    "                Generate an attention map and context-rich embedding that combines both sentences.\n",
    "        \"\"\"\n",
    "\n",
    "        decoders_self_attention_out = self.self_decoder_attention(\n",
    "            Q=target_sequence_position_embedding,\n",
    "            K=target_sequence_position_embedding,\n",
    "            V=target_sequence_position_embedding,\n",
    "        )\n",
    "\n",
    "        ## Resicual layer:\n",
    "        layer_normed_self_att = nn.LayerNorm(\n",
    "            decoders_self_attention_out + decoders_self_attention_out\n",
    "        )\n",
    "\n",
    "        encoder_decoder_attention = self.encoder_decoder_attention(\n",
    "            Q=layer_normed_self_att,\n",
    "            K=encoders_multihead_att_out,\n",
    "            V=encoders_multihead_att_out,\n",
    "        )\n",
    "\n",
    "        layer_normed_enc_dec_att = nn.LayerNorm(encoder_decoder_attention + layer_normed_self_att)\n",
    "\n",
    "        x_res1 = nn.LayerNorm(nn.ReLU(self.linear1(x)) + layer_normed_enc_dec_att)\n",
    "\n",
    "        x_res2 = nn.LayerNorm(nn.ReLU(self.linear1(x_res1)) + x_res1)\n",
    "\n",
    "        x_res3 = nn.LayerNorm(nn.ReLU(self.linear1(x_res2)) + x_res2)\n",
    "\n",
    "        return x_res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransform(nn.Module):\n",
    "    def __init__(self, num_decoders, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.decoder_list = [DecoderLayer() for i in range(num_decoders)]\n",
    "        self.stacked_decoders = nn.ModuleList([decoder for decoder in self.decoder_list])\n",
    "    def forward(self, x):\n",
    "        for decoder in self.self.stacked_decoders:\n",
    "            x = decoder(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
